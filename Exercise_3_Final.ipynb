{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importierung der Programmbibliotheken\n",
    "\n",
    "Die ```json``` Programmbibliothek ermöglicht das Nutzen von Funktionen, die das Bearbeiten von .json Dateien erleichtert.  \n",
    "Die ```requests``` Programmbibliothek erlaubt das einfache Ausgeben des HTML-Codes von Webseiten.  \n",
    "Die ```csv``` Programmbibliothek ermöglicht das Nutzen von Funktionen, die das Bearbeiten von CSV Dateien erleichtert.    \n",
    "Die ```re```(regex) Programmbibliothek ermöglicht das komplexe Suchen von Strings.  \n",
    "Die Programmbibliothek ```pprint``` hilft, komplette Datenstrukturen übersichtlicher darzustellen.\n",
    "Die Programmbibliothek ```BeatifulSoup``` hilft unter anderem, HTML und XML-Codes zu lesen und zu strukturieren.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import re\n",
    "import pprint\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition der Funktionen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion 1 ( \"Suche\" )\n",
    "\n",
    "Die Funktion ```Suche``` ist dafür da, den vorsortierten HTML Code nach für uns nützlichen Informationen zu durchsuchen.  \n",
    "Die Übergabeparameter ```Suche1,Suche2,Suche3``` sind RegEx-Angaben zur Verfeinerung und Informationsgewinnung aus dem HTML Code.  \n",
    "```infotable``` ist ein Übergabeparameter, der den vor verfeinerten HTML Code übergibt.  \n",
    "```nan``` ist die Bezeichnung, falls die RegEx-Suche nichts ergibt. Der Parameter muss übergeben werden, da die ```strings``` teilsweise noch bearbeitet werden.\n",
    "\n",
    "\n",
    "Die ersten drei Anweisungen der Funktion durchsucht einen ```string``` nach einer RegEx-Anweisung und speichert dies in eine neue Variable. Die ```if``` Abfrage kontrolliert, ob das Ergebnis der letzten Suche eine leere Liste ist. Ist dies der Fall, hat die Suche keine Ergebnisse. Dadurch wird die ```nan``` Variable angehangen.  \n",
    "Der ```return``` Value der Funktion ist ```info```. Dies ist eine Liste der Ergebnisse oder der Listeneintrag ```nan``` ist nicht vorhanden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Suche(Suche1,Suche2,Suche3,infotable,nan):\n",
    "    suchergeb1 = re.findall(Suche1,str(infotable))\n",
    "    suchergeb2 = re.findall(Suche2,str(suchergeb1))\n",
    "    info = re.findall(Suche3,str(suchergeb2))  \n",
    "    \n",
    "    if info == []:\n",
    "        \n",
    "        info = [nan]\n",
    "        \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion 2 ( \"writecsv\" )\n",
    "\n",
    "Die Funktion ```writeCSV``` zum Schreiben einer CSV Datei.  \n",
    "Der Übergabe Parameter ```namecountsschar``` ist ein String, dessen Inhalt der gewünschte Name ist.  \n",
    "Beim Übergabeparameter ```Dic``` wird ein Dictionary übergeben, das in eine CSV Datei gespeichert werden soll.  \n",
    "In der ersten Anweisung der Funktion wird ```fieldsdialog``` definiert und je nach Benennung verändern sich die Feldnamen der CSV Datei.  \n",
    "Die ```with open``` Funktion öffnet Dateien, solange sie in ihrer Syntax sind.  \n",
    "Als Parameter werden das ```w``` übergeben, welches für ```write``` steht und die geöffnete Datei zu schreiben ermöglicht und ```newline=\"\"```, welches bestimmt, was nach einem Absatz passiert.  \n",
    "Durch den Ausdruck wird ```writer``` ein csv.writer Objekt und übergibt die oben genannten Parameter an.\n",
    "```writer.writerow``` Hier wird die erste Zeile definiert. (Mit den Feldnamen die oben definiert wurden)\n",
    "```writer.writerows```(zip(Dic.keys(),Dic.values())) Diese Anweisung schreibt die Werte des Dictionary in die csv Datei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writecsv(namecountsschar, Dic):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fieldsdialog = ['Länder','Links']\n",
    "    \n",
    "    \n",
    "    with open(namecountsschar, \"w\", newline=\"\") as csvfile:\n",
    "        \n",
    "        writer = csv.writer(csvfile)\n",
    "        \n",
    "        writer.writerow(fieldsdialog)\n",
    "        \n",
    "        writer.writerows(zip(Dic.keys(),Dic.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs der Tabelle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```jsonFilePath``` ist der Name der ```.json``` Datei, indem das Ergebnis gespeichert werden soll.  \n",
    "Die Variable ```website_url``` ist das Ergebnis einer Anfrage mithilfe der ```requests.get``` Funktion, welches den Quellcode der Wikipedia Seite \"List of countries by meat consumption\" abfragt. \n",
    "```.text``` sorgt dafür, dass es in einen ```string``` umgewandelt wird und es gelesen werden kann.  \n",
    "Mithilfe der ```BeatifulSoup``` Funktion wird der ```string``` in einen ```BeautifulSoup``` umgewandelt, damit der Code der Tabelle herausgefiltert werden kann.  \n",
    "Danach wird der Code der Tabelle nach internen Wikipedia-Links durchsucht. Die Schleife geht die Liste der internen Wikipedialinks durch und liest diese nach und nach in eine ```dictionary``` ein bestehend aus den Ländernamen und der gesamten URL.  \n",
    "Die URL setzt sich zusammen aus der Wikipedia Seite: \"https://www.wikipedia.org/\" und dem internen Link abzüglich der letzten beiden Leerzeichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonFilePath = \"exercise3.json\"\n",
    "\n",
    "website_url = requests.get(\"https://en.wikipedia.org/wiki/List_of_countries_by_meat_consumption\").text\n",
    "\n",
    "soup = BeautifulSoup(website_url,\"html.parser\")\n",
    "\n",
    "countries = soup.find(\"table\",{\"class\":\"wikitable\"})\n",
    "\n",
    "b = re.findall(r\"/wiki/.*\\\" \",str(countries))\n",
    "\n",
    "Dic = {}\n",
    "\n",
    "for i in b:\n",
    "    \n",
    "    Dic[i[6:-2]] = \"https://en.wikipedia.org\" + i[:-2]\n",
    "\n",
    "writecsv(\"Listelinks.csv\",Dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infobox Informationen zu JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Schleife geht das ```dictionary``` Stück für Stück durch und gibt den ```key``` Attribut aus. Mithilfe des ```key``` Attributs wird aus dem ```dictionary``` die URL aufgerufen und die zwei darauf folgenden Schritte wie oben wiederholt.  \n",
    "Im nächsten Schritt wird die bereits oben definierte Funktion ```Suche``` genutzt, um die Größe der Länder zu extrahieren. Die erste RegEx-Funktion extrahiert den gesamten Quellcode zwischen dem Auftauchen von \"Area\" und dem Ort, an dem die Informationen gespeichert sind.  \n",
    "Die Extraktion der Hauptstadt ist ein Sonderfall, da diese nur zwei Schritte benötigt. Deswegen kann die Funktion ohne weiteres genutzt werden.  \n",
    "Die Bevölkerungszahl und die Bevölkerung werden wie oben extrahiert.  \n",
    "Das ```dictionary```, welches die Ergebnisse beinhaltet, hat als ```key``` Attribut den Ländernamen und als ```value``` Attribut weitere dictionaries mit den vorher extrahierten Informationen und deren Bezeichnungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-25c3f2dbd70e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mwebsite_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwebsite_url\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0minfotable\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"table\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"infobox geography vcard\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_feed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\__init__.py\u001b[0m in \u001b[0;36m_feed\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m         \u001b[1;31m# Close out any unfinished strings and close all the open tags.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m             \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mHTMLParseError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mfeed\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \"\"\"\n\u001b[0;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrawdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgoahead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mgoahead\u001b[1;34m(self, end)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'<'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mstarttagopen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# < + letter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_starttag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"</\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_endtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\html\\parser.py\u001b[0m in \u001b[0;36mparse_starttag\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_startendtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_starttag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCDATA_CONTENT_ELEMENTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_cdata_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\bs4\\builder\\_htmlparser.py\u001b[0m in \u001b[0;36mhandle_starttag\u001b[1;34m(self, name, attrs, handle_empty_element)\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0mattr_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[0mattrvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\"\"'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;31m#print \"START\", name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in Dic:\n",
    "    \n",
    "    Captial = \" \"\n",
    "    Area = \" \"\n",
    "    Demonym = \" \"\n",
    "    Pop = \" \"\n",
    "    \n",
    "    website_url = requests.get(Dic[i]).text\n",
    "    soup = BeautifulSoup(website_url,\"html.parser\")\n",
    "    infotable =  soup.find(\"table\", class_=\"infobox geography vcard\")\n",
    "    \n",
    "     \n",
    "    Area = Suche(r\"Area.+?(?=</td>)\",r\"</th><td>(.+?<sup)\",r\"[0-9,–]{2,12}\",infotable,\"nan\")\n",
    "       \n",
    "    c = re.findall(r\"Capital.+?(?=</a>)\",str(infotable))\n",
    "    Captial = re.findall(r\"title=\\\"(.+?)\\\"\",str(c))\n",
    "    \n",
    "    if Captial == []:\n",
    "        \n",
    "        Captial = [\"nan\"]\n",
    "    \n",
    "    Pop = Suche(r\"Population.+?(?=</a>)\",r\"</th><td>(.+?<sup)\",r\"[0-9,]{2,15}<sup|[0-9,]{2,15} <sup\",infotable,\"nan1234\")\n",
    "    \n",
    "    \n",
    "    Demonym = Suche(r\"Demonym.+?(?=</td>)\",r\"title=\\\"[^\\\"Demonym\\\"](.{1,40}a>)\",r\"\\\"(.+?)<\\/a\",infotable,\"1nan\")           \n",
    "    \n",
    "    \n",
    "    Dic[i] = {\n",
    "              \"Capital City\" : str(Captial[0]),\n",
    "              \"Area\" : str(Area[0]) +\"km^2\",\n",
    "              \"Demonym\" : str(Demonym[0])[1:],\n",
    "              \"Population\" : str(Pop[0])[:-4]\n",
    "              }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "with open(jsonFilePath, 'w') as jsonFile:\n",
    "    jsonFile.write(json.dumps(Dic,indent=4))    \n",
    "    \n",
    "# pprint.pprint(Dic) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
